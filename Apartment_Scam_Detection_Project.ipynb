{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "# import xgboost as xgb\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Cleaning for Housing Listings\n",
    "\n",
    "This notebook is dedicated to cleaning the housing dataset. The following tasks are performed:\n",
    "1. Loading the dataset.\n",
    "5. Renaming columns for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_housing_data = pd.read_csv(\"data/PrimaryDataset-MLS-RentalProperties.csv\") # load the dataset\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(raw_housing_data.shape)\n",
    "raw_housing_data.info()\n",
    "\n",
    "# Display the first few rows of the dataset to inspect it\n",
    "raw_housing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\n",
    "    'List Price', 'Bedrooms Total', 'Bathrooms Total', 'Living Area', \n",
    "    'MLS Area Major', 'Year Built', \n",
    "    'Lot Size Acres', 'Days on Market', 'Non-Representative Compensation',\n",
    "    'Waterfront YN', 'Garage YN', 'Stories Total', 'Stories', 'Bedrooms Total', 'Bathrooms Total',\n",
    "    'Bathrooms Full', 'Bathrooms Half', 'Garage YN', 'Garage Spaces', 'Original List Price', 'Latitude', 'Longitude', 'Rooms', 'Features'\n",
    "]\n",
    "\n",
    "numerical_columns = [\n",
    "    'List Price', 'Bedrooms Total', 'Bathrooms Total', 'Living Area', 'MLS Area Major', 'Year Built', 'Lot Size Acres', 'Days on Market', 'Non-Representative Compensation',\n",
    "    'Stories Total', 'Stories', 'Bathrooms Full', 'Bathrooms Half', 'Garage Spaces', 'Original List Price', 'Latitude', 'Longitude'\n",
    "]\n",
    "\n",
    "print(len(numerical_columns))\n",
    "\n",
    "# Drop the irrelevant columns\n",
    "cleaned_housing_data = raw_housing_data[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "cleaned_housing_data = cleaned_housing_data.dropna()\n",
    "\n",
    "# Check for duplicate columns and remove them\n",
    "cleaned_housing_data = cleaned_housing_data.loc[:, ~cleaned_housing_data.columns.duplicated()]\n",
    "\n",
    "# Display the dataset shape and head after these operations\n",
    "print(cleaned_housing_data.shape)\n",
    "cleaned_housing_data.info()\n",
    "cleaned_housing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to save the CSV\n",
    "file_path = 'data/cleaned_housing_data.csv'\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_path):\n",
    "    cleaned_housing_data.to_csv(file_path, index=False)\n",
    "    print(f\"File saved as {file_path}\")\n",
    "else:\n",
    "    print(f\"File {file_path} already exists. No action taken.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Principal Component Analysis (PCA) and Anomaly Detection\n",
    "\n",
    "In this section, we apply PCA for dimensionality reduction and anomaly detection on housing data.\n",
    "We will retain 95% of variance and also explore anomaly detection using DBSCAN and Isolation Forest.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Fit PCA and Transform the Data\n",
    "Here, we fit PCA to the scaled dataset and retain 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit the PCA (retaining 95% of variance)\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(cleaned_housing_data)\n",
    "\n",
    "print(x_scaled.shape)\n",
    "pca_model = PCA(n_components=0.95)\n",
    "x_pca = pca_model.fit_transform(x_scaled)  # 'x_scaled' is the scaled input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Create a DataFrame for the Principal Components\n",
    "\n",
    "We create a DataFrame that contains the principal components for each data point.\n",
    "Each component captures the variance in the original features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create column names for PCA components\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])]\n",
    "\n",
    "# Create a DataFrame for the PCA-transformed data\n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "pca_df.head()\n",
    "\n",
    "model1 = IsolationForest(n_estimators=100, max_samples='auto', contamination='auto', random_state=12)\n",
    "\n",
    "model1.fit(x_pca)\n",
    "\n",
    "predict = model1.predict(x_pca)\n",
    "anomalies = x_scaled[predict == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit the PCA as before\n",
    "pca = PCA(n_components=0.95)  # Retaining 95% of variance\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# Step 2: Create a DataFrame for the principal components\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])] \n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "\n",
    "# Step 3: Get the contributions of each original feature to each principal component\n",
    "loadings = pca.components_.T  # Transpose to get original features as rows\n",
    "print(loadings.shape)\n",
    "print(len(numerical_columns))\n",
    "contributions_df = pd.DataFrame(loadings, index=numerical_columns, columns=pca_columns)\n",
    "\n",
    "\n",
    "# Step 4: Get the top contributing feature for each principal component (for labeling purposes)\n",
    "top_features_per_pc = contributions_df.abs().idxmax()\n",
    "\n",
    "# Step 5: Create a box plot and label each principal component by its strongest contributing original feature\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=pca_df)\n",
    "\n",
    "# Rotate the text diagonally and align it so it reads from top to bottom\n",
    "plt.xticks(range(len(top_features_per_pc)), top_features_per_pc, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Title\n",
    "plt.title('Boxplot for Each Principal Component - Labeled by Top Contributing Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit the PCA as before\n",
    "pca = PCA(n_components=0.95)  # Retaining 95% of variance\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# Step 2: Create a DataFrame for the principal components\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])]\n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "\n",
    "# Step 3: Get the contributions of each original feature to each principal component\n",
    "loadings = pca.components_.T  # Transpose to get original features as rows\n",
    "contributions_df = pd.DataFrame(loadings, index=numerical_columns, columns=pca_columns)\n",
    "\n",
    "# Step 4: Get the top contributing feature for each principal component (for labeling purposes)\n",
    "top_features_per_pc = contributions_df.abs().idxmax()\n",
    "\n",
    "# Step 5: Add anomaly labels to the PCA DataFrame\n",
    "pca_df['anomaly'] = predict  # Anomalies (-1) and normal points (1)\n",
    "\n",
    "# Step 6: Parallel Coordinates Plot with PCA data and anomaly labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "parallel_coordinates(pca_df, 'anomaly', color=['blue', 'red'])\n",
    "\n",
    "# Step 7: Adjust the x-axis labels (principal components) with top contributing original feature names\n",
    "plt.xticks(range(len(top_features_per_pc)), top_features_per_pc, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Title\n",
    "plt.title('Parallel Coordinates Plot for PCA-Transformed Housing Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit the PCA as before\n",
    "pca = PCA(n_components=0.95)  # Retaining 95% of variance\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# Step 2: Create a DataFrame for the principal components\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])]\n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "\n",
    "# Step 3: Get the contributions of each original feature to each principal component\n",
    "loadings = pca.components_.T  # Transpose to get original features as rows\n",
    "contributions_df = pd.DataFrame(loadings, index=numerical_columns, columns=pca_columns)\n",
    "\n",
    "# Step 4: Get the top contributing feature for each principal component (for labeling purposes)\n",
    "top_features_per_pc = contributions_df.abs().idxmax()\n",
    "\n",
    "# Step 5: Filter the PCA data to show only anomalies (-1)\n",
    "anomalous_data = pca_df[predict == -1]\n",
    "\n",
    "# Step 6: Plot a heatmap of the anomalous entries (use PCA-transformed data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(anomalous_data, cmap='coolwarm', annot=False, linewidths=0.5)\n",
    "\n",
    "# Step 7: Adjust x-axis labels to reflect original features (diagonal or vertical)\n",
    "plt.xticks(range(len(top_features_per_pc)), top_features_per_pc, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Title\n",
    "plt.title('Heatmap of Anomalous Entries Across Principal Components (PCA-Transformed Data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit DBSCAN on the PCA-transformed data\n",
    "dbscan_model = DBSCAN(eps=0.5, min_samples=5)  # Adjust eps and min_samples based on your data\n",
    "dbscan_labels = dbscan_model.fit_predict(x_pca)\n",
    "\n",
    "# Step 2: Add DBSCAN cluster labels to the PCA DataFrame\n",
    "pca_df['dbscan_cluster'] = dbscan_labels\n",
    "\n",
    "# Step 3: Visualize the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=pca_df['dbscan_cluster'], palette='viridis')\n",
    "plt.title('DBSCAN Clustering on PCA-Transformed Data')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Cross-examine DBSCAN clusters with Isolation Forest results\n",
    "pca_df['anomaly_iforest'] = predict  # Add Isolation Forest anomaly labels (-1 for anomalies, 1 for normal)\n",
    "\n",
    "# Compare clusters and anomalies\n",
    "cross_examined = pca_df[(pca_df['dbscan_cluster'] != -1) & (pca_df['anomaly_iforest'] == -1)]\n",
    "\n",
    "print(f\"Number of points flagged by both DBSCAN and Isolation Forest: {len(cross_examined)}\")\n",
    "cross_examined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Over_30_Days_Flag (0 = 30 days or less, 1 = more than 90 days)\n",
    "cleaned_housing_data['Over_30_Days_Flag'] = cleaned_housing_data['Days on Market'].apply(lambda x: 1 if x > 30 else 0)\n",
    "\n",
    "# Save the updated dataset to a new CSV file with a better file name\n",
    "file_path = 'data/housing_data_over_30_days_flag.csv'\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_path):\n",
    "    cleaned_housing_data.to_csv(file_path, index=False)\n",
    "    print(f\"File saved as {file_path}\")\n",
    "else:\n",
    "    print(f\"File {file_path} already exists. No action taken.\")\n",
    "\n",
    "# Verify the first few rows of the updated dataset\n",
    "print(cleaned_housing_data[['Days on Market', 'Over_30_Days_Flag']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from math import radians, cos, sin, asin, sqrt, log\n",
    "\n",
    "# Important locationations for proximity scoring\n",
    "important_coordinates = [\n",
    "    {\"name\": \"University of North Florida\", \"latitude\": 30.2715, \"longitude\": -81.5094, \"weight\": 0.7727},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Flagler College\", \"latitude\": 29.8947, \"longitude\": -81.3145, \"weight\": 0.1182},  # St. Augustine, St. Johns County\n",
    "    {\"name\": \"St. Johns River State College\", \"latitude\": 29.6486, \"longitude\": -81.6417, \"weight\": 0.2955},  # Palatka, Putnam County\n",
    "    {\"name\": \"Edward Waters University\", \"latitude\": 30.3422, \"longitude\": -81.6794, \"weight\": 0.0455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Concorde Career Institute\", \"latitude\": 30.3374, \"longitude\": -81.5546, \"weight\": 0.0227},  # Jacksonville, Duval County\n",
    "    {\"name\": \"First Coast Technical College\", \"latitude\": 29.8922, \"longitude\": -81.3305, \"weight\": 0.0182},  # St. Augustine, St. Johns County\n",
    "    {\"name\": \"Jacksonville University\", \"latitude\": 30.3532, \"longitude\": -81.6068, \"weight\": 0.2045},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Jones Technical Institute\", \"latitude\": 30.2449, \"longitude\": -81.5322, \"weight\": 0.0182},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Tulsa Welding School\", \"latitude\": 30.3385, \"longitude\": -81.5637, \"weight\": 0.0136},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Chamberlain University-Florida\", \"latitude\": 30.2598, \"longitude\": -81.5904, \"weight\": 0.0409},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Fortis College-Orange Park\", \"latitude\": 30.1785, \"longitude\": -81.7079, \"weight\": 0.0318},  # Orange Park, Clay County\n",
    "    {\"name\": \"Florida State College at Jacksonville\", \"latitude\": 30.3322, \"longitude\": -81.6557, \"weight\": 1.0000},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Trinity Baptist College\", \"latitude\": 30.2395, \"longitude\": -81.7802, \"weight\": 0.0227},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Keiser University\", \"latitude\": 30.3326, \"longitude\": -81.6562, \"weight\": 0.0455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Heritage Institute\", \"latitude\": 30.2033, \"longitude\": -81.5837, \"weight\": 0.0182},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Embry-Riddle Aeronautical University\", \"latitude\": 29.1880, \"longitude\": -81.0479, \"weight\": 0.4091},  # Daytona Beach, Volusia County\n",
    "    {\"name\": \"Naval Air Station Jacksonville\", \"latitude\": 30.2358, \"longitude\": -81.6800, \"weight\": 0.9545},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Naval Station Mayport\", \"latitude\": 30.3915, \"longitude\": -81.4245, \"weight\": 0.5455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Camp Blanding Joint Training Center\", \"latitude\": 29.9693, \"longitude\": -81.9840, \"weight\": 0.6818},  # Clay County\n",
    "    {\"name\": \"Marine Corps Blount Island Command\", \"latitude\": 30.4111, \"longitude\": -81.5059, \"weight\": 0.1364},  # Jacksonville, Duval County\n",
    "]\n",
    "\n",
    "# Haversine formula to calculate the distance between two points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6371 * c  # Radius of Earth in kilometers\n",
    "    return km\n",
    "\n",
    "# Function to calculate suspicion score based on proximity to important locationations\n",
    "def calculate_weighted_suspiciousness(row, important_locationations):\n",
    "    listing_lat = row['Latitude']\n",
    "    listing_lon = row['Longitude']\n",
    "    \n",
    "    total_suspiciousness = 0\n",
    "    max_distance = 50  # Reasonable commuting distance\n",
    "    \n",
    "    for location in important_locationations:\n",
    "        dist = haversine(listing_lat, listing_lon, location['latitude'], location['longitude'])\n",
    "        \n",
    "        if dist <= max_distance:\n",
    "            location_suspiciousness = location['weight'] * (1 / (log(dist + 1) + 1))\n",
    "            total_suspiciousness += location_suspiciousness\n",
    "    \n",
    "    return total_suspiciousness\n",
    "\n",
    "# Apply the suspicion calculation to each listing in the dataset\n",
    "cleaned_housing_data['Distance Suspiciousness'] = cleaned_housing_data.apply(lambda row: calculate_weighted_suspiciousness(row, important_coordinates), axis=1)\n",
    "\n",
    "# Filter out rows where latitude or longitude is missing\n",
    "filtered_data = cleaned_housing_data[['Latitude', 'Longitude', 'Distance Suspiciousness']].dropna()\n",
    "\n",
    "# Create a map centered around Duval County\n",
    "m = folium.Map(locationation=[30.3322, -81.6557], zoom_start=10)\n",
    "\n",
    "# Create a list of coordinates and weights (suspicion scores) for the heatmap\n",
    "heatmap_data = [[row['Latitude'], row['Longitude'], row['Distance Suspiciousness']] for index, row in filtered_data.iterrows()]\n",
    "\n",
    "# Add the heatmap layer with suspicion scores\n",
    "HeatMap(heatmap_data, max_value=1, radius=15, blur=10).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('suspicion_score_heatmap.html')\n",
    "\n",
    "# If running in a notebook, display the map inline\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_coordinates = [\n",
    "    # Colleges and universities are assigned weights based on their student population, \n",
    "    # normalized against the largest population in this list (Florida State College at Jacksonville with 22,000 students).\n",
    "    {\"name\": \"University of North Florida\", \"latitude\": 30.2715, \"longitude\": -81.5094, \"weight\": 0.7727},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Flagler College\", \"latitude\": 29.8947, \"longitude\": -81.3145, \"weight\": 0.1182},  # St. Augustine, St. Johns County\n",
    "    {\"name\": \"St. Johns River State College\", \"latitude\": 29.6486, \"longitude\": -81.6417, \"weight\": 0.2955},  # Palatka, Putnam County\n",
    "    {\"name\": \"Edward Waters University\", \"latitude\": 30.3422, \"longitude\": -81.6794, \"weight\": 0.0455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Concorde Career Institute\", \"latitude\": 30.3374, \"longitude\": -81.5546, \"weight\": 0.0227},  # Jacksonville, Duval County\n",
    "    {\"name\": \"First Coast Technical College\", \"latitude\": 29.8922, \"longitude\": -81.3305, \"weight\": 0.0182},  # St. Augustine, St. Johns County\n",
    "    {\"name\": \"Jacksonville University\", \"latitude\": 30.3532, \"longitude\": -81.6068, \"weight\": 0.2045},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Jones Technical Institute\", \"latitude\": 30.2449, \"longitude\": -81.5322, \"weight\": 0.0182},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Tulsa Welding School\", \"latitude\": 30.3385, \"longitude\": -81.5637, \"weight\": 0.0136},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Chamberlain University-Florida\", \"latitude\": 30.2598, \"longitude\": -81.5904, \"weight\": 0.0409},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Fortis College-Orange Park\", \"latitude\": 30.1785, \"longitude\": -81.7079, \"weight\": 0.0318},  # Orange Park, Clay County\n",
    "    {\"name\": \"Florida State College at Jacksonville\", \"latitude\": 30.3322, \"longitude\": -81.6557, \"weight\": 1.0000},  # Jacksonville, Duval County (Largest population: 22,000 students)\n",
    "    {\"name\": \"Trinity Baptist College\", \"latitude\": 30.2395, \"longitude\": -81.7802, \"weight\": 0.0227},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Keiser University\", \"latitude\": 30.3326, \"longitude\": -81.6562, \"weight\": 0.0455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Heritage Institute\", \"latitude\": 30.2033, \"longitude\": -81.5837, \"weight\": 0.0182},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Embry-Riddle Aeronautical University\", \"latitude\": 29.1880, \"longitude\": -81.0479, \"weight\": 0.4091},  # Daytona Beach, Volusia County\n",
    "    \n",
    "    # Military bases have their weights calculated based on a combination of permanent and transient personnel. \n",
    "    # We estimate the size of the population the base supports and normalize it against the highest value.\n",
    "    {\"name\": \"Naval Air Station Jacksonville\", \"latitude\": 30.2358, \"longitude\": -81.6800, \"weight\": 0.9545},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Naval Station Mayport\", \"latitude\": 30.3915, \"longitude\": -81.4245, \"weight\": 0.5455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Camp Blanding Joint Training Center\", \"latitude\": 29.9693, \"longitude\": -81.9840, \"weight\": 0.6818},  # Clay County\n",
    "    {\"name\": \"Marine Corps Blount Island Command\", \"latitude\": 30.4111, \"longitude\": -81.5059, \"weight\": 0.1364},  # Jacksonville, Duval County\n",
    "]\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt, log\n",
    "\n",
    "# Haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6371 * c  # Earth radius in kilometers\n",
    "    return km\n",
    "\n",
    "# Adjusted suspiciousness calculation\n",
    "def calculate_weighted_suspiciousness(row, important_locationations, max_distance=50, scaling_factor=1):\n",
    "    listing_lat = row['Latitude']\n",
    "    listing_lon = row['Longitude']\n",
    "    \n",
    "    # Initialize the suspiciousness score\n",
    "    total_suspiciousness = 0\n",
    "    baseline_suspiciousness = 0.05  # To account for listings too far from important locationations\n",
    "    \n",
    "    # Loop through important locationations and compute suspiciousness\n",
    "    for location in important_locationations:\n",
    "        dist = haversine(listing_lat, listing_lon, location['latitude'], location['longitude'])\n",
    "        \n",
    "        # Only compute suspiciousness for distances within the max limit\n",
    "        if dist <= max_distance:\n",
    "            # Apply scaling factor to dampen the effect of distance logarithmically\n",
    "            location_suspiciousness = location['weight'] * (1 / (log(dist + 1) + scaling_factor))\n",
    "            total_suspiciousness += location_suspiciousness\n",
    "    \n",
    "    # Add baseline score if total suspiciousness is very low\n",
    "    total_suspiciousness = max(baseline_suspiciousness, total_suspiciousness)\n",
    "    \n",
    "    return total_suspiciousness\n",
    "\n",
    "# Apply the function to each row and create a new column 'Distance Suspiciousness'\n",
    "cleaned_housing_data['Distance Suspiciousness'] = cleaned_housing_data.apply(lambda row: calculate_weighted_suspiciousness(row, important_coordinates), axis=1)\n",
    "cleaned_housing_data['Distance Suspiciousness'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cleaned_housing_data['Distance Suspiciousness'], bins=30, edgecolor='black')\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Distance Suspiciousness')\n",
    "plt.xlabel('Suspiciousness Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate column names\n",
    "print(\"Checking for duplicate column names...\")\n",
    "if cleaned_housing_data.columns.duplicated().any():\n",
    "    print(\"Duplicate columns detected:\")\n",
    "    # Get a list of all columns with duplicates\n",
    "    duplicate_columns = cleaned_housing_data.columns[cleaned_housing_data.columns.duplicated()].tolist()\n",
    "    print(\"Duplicate columns:\", duplicate_columns)\n",
    "    \n",
    "    # Remove duplicates by dropping the duplicated columns\n",
    "    cleaned_housing_data = cleaned_housing_data.location[:, ~cleaned_housing_data.columns.duplicated()]\n",
    "    print(\"Duplicate columns removed. Proceeding with the calculation.\")\n",
    "else:\n",
    "    print(\"No duplicate columns found. Proceeding with the calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "housing_data_ratios = cleaned_housing_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with calculated columns\n",
    "df_price_metrics = pd.DataFrame()\n",
    "\n",
    "# Assuming 'List Price', 'Bedrooms Total', 'Bathrooms Full', 'Bathrooms Total', 'Stories', 'Garage Spaces', 'Living Area', 'Lot Size Acres', 'Year Built' columns exist in your dataset\n",
    "\n",
    "df_price_metrics['Price per Bedroom'] = cleaned_housing_data['List Price'] / cleaned_housing_data['Bedrooms Total']\n",
    "df_price_metrics['Price per Full Bathroom'] = cleaned_housing_data['List Price'] / cleaned_housing_data['Bathrooms Full']\n",
    "df_price_metrics['Price per Total Bathroom'] = cleaned_housing_data['List Price'] / cleaned_housing_data['Bathrooms Total']\n",
    "df_price_metrics['Price per Story'] = cleaned_housing_data['List Price'] / cleaned_housing_data['Stories']\n",
    "df_price_metrics['Price per Garage Space'] = cleaned_housing_data['List Price'] / cleaned_housing_data['Garage Spaces']\n",
    "df_price_metrics['Price per Living Area'] = cleaned_housing_data['List Price'] / cleaned_housing_data['Living Area']\n",
    "df_price_metrics['Price per Lot Size Acre'] = cleaned_housing_data['List Price'] / cleaned_housing_data['Lot Size Acres']\n",
    "df_price_metrics['Price per Year Built'] = cleaned_housing_data['List Price'] / cleaned_housing_data['Year Built']\n",
    "\n",
    "# Display the first few rows of the new dataframe\n",
    "df_price_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_metrics['Price per Story'].fillna(cleaned_housing_data['List Price'], inplace=True)\n",
    "\n",
    "# Handle Garage Spaces: Instead of 0, fill NaN with the median value\n",
    "df_price_metrics['Price per Garage Space'].fillna(df_price_metrics['Price per Garage Space'].median(), inplace=True)\n",
    "\n",
    "# Replace infinity for Price per Acre with the median value\n",
    "df_price_metrics['Price per Lot Size Acre'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_price_metrics['Price per Lot Size Acre'].fillna(df_price_metrics['Price per Lot Size Acre'].median(), inplace=True)\n",
    "\n",
    "df_price_metrics.fillna(df_price_metrics.median(), inplace=True)\n",
    "\n",
    "df_price_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter the dataset for relevant columns (replace with actual column names if needed)\n",
    "df_phone_name = raw_housing_data[['Listing Agent', 'Agency Phone']]\n",
    "\n",
    "# Step 2: Group by 'Phone Number' and aggregate the unique 'Agent Name' values\n",
    "duplicates = df_phone_name.groupby('Agency Phone')['Listing Agent'].nunique()\n",
    "\n",
    "# Step 3: Filter for phone numbers that are associated with more than one unique agent name\n",
    "suspicious_numbers = duplicates[duplicates > 1]\n",
    "\n",
    "# Step 4: Flag listings that contain suspicious phone numbers\n",
    "def flag_suspicious_phone(row):\n",
    "    if row['Agency Phone'] in suspicious_numbers.index:\n",
    "        return 1  # Flag as suspicious\n",
    "    return 0  # Not suspicious\n",
    "\n",
    "# Step 5: Apply the function to the raw_housing_data DataFrame\n",
    "raw_housing_data['is_phone_suspicious'] = raw_housing_data.apply(flag_suspicious_phone, axis=1)\n",
    "\n",
    "# Step 6: Display flagged listings\n",
    "suspicious_phone_listings = raw_housing_data[raw_housing_data['is_phone_suspicious'] == 1]\n",
    "print(suspicious_phone_listings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_words = [\n",
    "    'urgent', 'alert', 'wire transfer', 'guaranteed', 'free',\n",
    "    'cash only', 'as seen on', 'limited time', 'don’t miss out',\n",
    "    'risk-free', 'act now', 'exclusive', 'once in a lifetime',\n",
    "   'contact now', 'no credit check', 'easy approval','Foreclosure',\n",
    "     \"no deposit required\", \"move-in specials\", \"free month rent\", \"lease takeover\", \"rent-to-own\", \n",
    "    \"pre-approval needed\", \"urgent rental\", \"hurry, limited time offer\", \"cash only, no checks\", \n",
    "    \"first month free\", \"no background check\", \"instant approval\", \"no credit history needed\", \n",
    "    \"temporary housing\", \"assume the lease\", \"short-term rental\", \"virtual tour only\", \n",
    "    \"sublease opportunity\", \"guaranteed approval\", \"utilities included\", \"all bills paid\", \n",
    "   \"no application fee\", \"get approved today\", \"house sitting\", \"unbelievably low rent\", \n",
    "    \"no lease required\", \"instant income\", \"non-refundable deposit\", \"limited properties available\", \n",
    "    \"you won't believe the price\", \"exclusive listings\", \"flexible terms\", \"unforeseen circumstances\", \n",
    "    \"background check waived\", \"contact immediately\", \"first come, first served\", \"urgent need to rent\", \n",
    "    \"newly renovated\", \"don’t get left out\", \"act fast before it’s gone\", \"scam-free guarantee\", \n",
    "    \"friendly landlord\", \"best value rental\", \"quick approval process\", \"no hassle, no fees\", \n",
    "    \"all-inclusive rental\", \"hidden gem\", \"affordable living\", \"ideal for students\", \n",
    "    \"rent today, move in tomorrow\"\n",
    "]\n",
    "\n",
    "def flag_suspicious_listings(row):\n",
    "    # Join relevant columns into one text\n",
    "    text = f\"{row['Features']}\".lower()  # Combine and convert to lowercase\n",
    "    # Check for suspicious words\n",
    "    for word in suspicious_words:\n",
    "        if word in text:\n",
    "            return 1  # Flag as suspicious\n",
    "    return 0  # Not suspicious\n",
    "\n",
    "# Assuming Features is the column to check\n",
    "raw_housing_data['is_suspicious'] = raw_housing_data.apply(flag_suspicious_listings, axis=1)\n",
    "\n",
    "# Display flagged listings\n",
    "suspicious_listings = raw_housing_data[raw_housing_data['is_suspicious'] == 1]\n",
    "print(suspicious_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Start with the base cleaned DataFrame\n",
    "final_df = cleaned_housing_data.copy()\n",
    "\n",
    "# Step 2: Add the new features you created\n",
    "# Assuming you have DataFrames or Series for these additional features, like df_price_metrics, proximity_scores, etc.\n",
    "\n",
    "# Example: Add price metrics\n",
    "final_df = pd.concat([final_df, df_price_metrics], axis=1)\n",
    "\n",
    "# Example: Add scammy words flag (assuming 'is_suspicious' is a column in the raw_housing_data)\n",
    "final_df['suspicious_diction'] = raw_housing_data['is_phone_suspicious']\n",
    "\n",
    "final_df['phone_is_suspicious'] =  raw_housing_data['pho']\n",
    "# Step 3: Clean the final DataFrame\n",
    "# Handle NaN values, infinity, and other inconsistencies\n",
    "final_df.fillna(final_df.median(), inplace=True)  # Fill NaNs with the median of each column\n",
    "\n",
    "\n",
    "# Step 4: Check the final DataFrame\n",
    "final_df.info()  # To see if everything looks good\n",
    "final_df.head()  # To preview the first few rows\n",
    "\n",
    "\n",
    "file_path = 'data/final_training_set.csv'\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_path):\n",
    "    final_df.to_csv(file_path, index=False)\n",
    "    print(f\"File saved as {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select relevant features for anomaly detection\n",
    "features = ['List Price', 'Bedrooms Total', 'Bathrooms Total', 'Living Area', \n",
    "            'Lot Size Acres', 'Distance Suspiciousness', 'Price per Bedroom', \n",
    "            'Price per Living Area', 'Price per Garage Space']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 1: Replace infinite values with NaN\n",
    "final_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Step 2: Fill NaN values (e.g., with the median of each column)\n",
    "final_df.fillna(final_df.median(), inplace=True)\n",
    "\n",
    "X_scaled = scaler.fit_transform(final_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit Isolation Forest model\n",
    "isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "final_df['isolation_forest_flag'] = isolation_forest.fit_predict(X_scaled)\n",
    "\n",
    "# Anomalies are flagged as -1\n",
    "anomalies_if = final_df[final_df['isolation_forest_flag'] == -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit LOF model\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "final_df['lof_flag'] = lof.fit_predict(X_scaled)\n",
    "\n",
    "# LOF also flags anomalies as -1\n",
    "anomalies_lof = final_df[final_df['lof_flag'] == -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Prepare a label where 0 is normal and 1 is anomaly\n",
    "final_df['anomaly_label'] = (final_df['suspicious_diction'] == 1).astype(int)\n",
    "\n",
    "# Prepare the DMatrix for XGBoost\n",
    "X = final_df[features]\n",
    "y = final_df['anomaly_label']\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "# Train the XGBoost model (we can treat it as a classification problem)\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'scale_pos_weight': len(y) / y.sum()  # Adjust for class imbalance\n",
    "}\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict anomaly scores (using threshold 0.5 to flag anomalies)\n",
    "final_df['xgboost_flag'] = (xgb_model.predict(dtrain) > 0.5).astype(int)\n",
    "\n",
    "# Flag XGBoost anomalies\n",
    "anomalies_xgb = final_df[final_df['xgboost_flag'] == 1]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine flags: Check if all three models flagged the same listing\n",
    "final_df['flagged_by_all'] = (\n",
    "    (final_df['isolation_forest_flag'] == -1) & \n",
    "    (final_df['lof_flag'] == -1)) #& \n",
    "    # (final_df['xgboost_flag'] == 1)\n",
    "#)\n",
    "\n",
    "# Get the listings flagged by all models\n",
    "anomalies_all_models = final_df[final_df['flagged_by_all']]\n",
    "\n",
    "# Display the results\n",
    "anomalies_all_models.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_df.index, final_df['List Price'], \n",
    "            c=final_df['isolation_forest_flag'], cmap='coolwarm', label='Anomaly Score')\n",
    "plt.title('Isolation Forest - Anomaly Detection based on List Price')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('List Price')\n",
    "plt.colorbar(label='Flag (-1: Anomaly, 1: Normal)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOF Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_df.index, final_df['Living Area'], \n",
    "            c=final_df['lof_flag'], cmap='coolwarm', label='Anomaly Score')\n",
    "plt.title('LOF - Anomaly Detection based on Living Area')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Living Area')\n",
    "plt.colorbar(label='Flag (-1: Anomaly, 1: Normal)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# XGBoost Probability Scatter Plot\n",
    "xgboost_probs = xgb_model.predict(dtrain)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_training_set.index, xgboost_probs, c=(xgboost_probs > 0.5).astype(int), cmap='coolwarm')\n",
    "plt.title('XGBoost - Predicted Anomaly Probabilities')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Anomaly Probability')\n",
    "plt.colorbar(label='Flag (1: Anomaly, 0: Normal)')\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Count how many were flagged by each model\n",
    "isolation_flags = set(final_df.index[final_df['isolation_forest_flag'] == -1])\n",
    "lof_flags = set(final_df.index[final_df['lof_flag'] == -1])\n",
    "\n",
    "# Create a Venn diagram comparing the results of Isolation Forest and LOF\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn = venn2([isolation_flags, lof_flags], \n",
    "             set_labels=('Isolation Forest', 'LOF'))\n",
    "\n",
    "plt.title('Venn Diagram of Anomalies Flagged by Isolation Forest and LOF')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_df.index, final_df['List Price'], \n",
    "            c=final_df['flagged_by_all'], cmap='coolwarm')\n",
    "plt.title('Listings Flagged as Anomalies by All Models')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('List Price')\n",
    "plt.colorbar(label='Flag (1: Anomaly by all models, 0: Normal)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of listings flagged as anomalies by either Isolation Forest or LOF\n",
    "anomalies_by_either = isolation_flags.union(lof_flags)\n",
    "anomalies_by_either_list = list(anomalies_by_either)\n",
    "\n",
    "# Filter the DataFrame to get the listings marked as anomalies by either model\n",
    "anomalous_listings = final_df.loc[anomalies_by_either_list]\n",
    "\n",
    "# Print the listings marked as anomalies by either model\n",
    "print(\"Listings flagged as anomalies by either Isolation Forest or LOF:\")\n",
    "print(anomalous_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming final_df is your updated DataFrame\n",
    "# Step 1: Select only the numerical features from final_df for PCA\n",
    "numeric_features = final_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Step 2: Extract the numeric features from final_df\n",
    "X = final_df[numeric_features].values\n",
    "\n",
    "# Step 3: Standardize the numeric features before applying PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Check the shape of X_scaled to confirm preprocessing\n",
    "print(X_scaled.shape)\n",
    "\n",
    "# Step 4: Apply PCA, retaining 95% of the variance\n",
    "pca = PCA(n_components=0.95)  # Retaining 95% variance\n",
    "\n",
    "x_pca = pca.fit_transform(X_scaled)  # Perform PCA on scaled data\n",
    "\n",
    "# Step 5: Create a DataFrame for the PCA-transformed data\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])]\n",
    "\n",
    "# Get the top contributing features for each principal component\n",
    "top_features_per_pc = []\n",
    "for i in range(pca.components_.shape[0]):\n",
    "    top_feature_index = np.argmax(np.abs(pca.components_[i]))\n",
    "    top_feature = numeric_features[top_feature_index]\n",
    "    top_features_per_pc.append(top_feature)\n",
    "\n",
    "# Create more descriptive principal component labels\n",
    "pca_columns_descriptive = [f'PC{i+1} ({top_features_per_pc[i]})' for i in range(len(pca_columns))]\n",
    "\n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "\n",
    "\n",
    "\n",
    "# Step 6: Visualize the explained variance for each principal component\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Get the loadings (contributions) of each original feature to each principal component\n",
    "loadings = pca.components_.T  # Transpose to get original features as rows\n",
    "contributions_df = pd.DataFrame(loadings, index=numeric_features, columns=pca_columns)\n",
    "\n",
    "# Step 5: Generate a heatmap of the contributions\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contributions_df, cmap='coolwarm', annot=True, fmt='.2f', linewidths=0.5)\n",
    "plt.title('Heatmap of Feature Contributions to Principal Components')\n",
    "plt.show()\n",
    "\n",
    "# Limit to a reasonable number of principal components (e.g., 10 for visualization)\n",
    "pca_df_limited = pca_df.iloc[:, :17]\n",
    "\n",
    "# Generate the boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=pca_df_limited)\n",
    "\n",
    "# Fix the xticks and labels\n",
    "plt.title('Boxplot of Principal Components')\n",
    "plt.xticks(rotation=90, ticks=np.arange(len(pca_df_limited.columns)), labels=pca_columns_descriptive[:17])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Step 6: Boxplot for each principal component\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# # pca_df.boxplot()\n",
    "# sns.boxplot(data=pca_df)\n",
    "\n",
    "# plt.title('Boxplot of Principal Components')\n",
    "# plt.xticks(rotation=90, ticks=np.arange(1, len(pca_columns_descriptive) + 1), labels=pca_columns_descriptive)\n",
    "# plt.show()\n",
    "\n",
    "#Step 7: Perform DBSCAN on the PCA-transformed data\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can adjust eps and min_samples based on your data\n",
    "dbscan_labels = dbscan.fit_predict(x_pca)\n",
    "\n",
    "# Step 8: Add the DBSCAN labels to the PCA DataFrame\n",
    "pca_df['DBSCAN_label'] = dbscan_labels\n",
    "\n",
    "# Step 9: Visualize the DBSCAN results using a scatter plot of the first two principal components\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='DBSCAN_label', data=pca_df, palette='coolwarm')\n",
    "plt.title('DBSCAN Clustering on Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top contributing features for each principal component\n",
    "top_features_per_pc = []\n",
    "for i in range(pca.components_.shape[0]):\n",
    "    top_feature_index = np.argmax(np.abs(pca.components_[i]))\n",
    "    top_feature = numeric_features[top_feature_index]\n",
    "    top_features_per_pc.append(top_feature)\n",
    "\n",
    "# Create more descriptive principal component labels\n",
    "pca_columns_descriptive = [f'PC{i+1} ({top_features_per_pc[i]})' for i in range(len(pca_columns))]\n",
    "\n",
    "# Update the heatmap with descriptive labels\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contributions_df, cmap='coolwarm', annot=True, fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Contribution to Principal Components', fontsize=16)\n",
    "plt.xlabel('Principal Components (Top Feature in Parentheses)', fontsize=12)\n",
    "plt.ylabel('Feature Names', fontsize=12)\n",
    "plt.xticks(ticks=np.arange(len(pca_columns_descriptive)) + 0.5, labels=pca_columns_descriptive, rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "# Use descriptive labels for the principal components in the explained variance plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Variance Explained by Principal Components', fontsize=16)\n",
    "plt.xlabel('Principal Components (Top Feature)', fontsize=12)\n",
    "plt.ylabel('Variance Ratio (Explained Variance %)', fontsize=12)\n",
    "plt.xticks(ticks=np.arange(1, len(pca_columns_descriptive) + 1), labels=pca_columns_descriptive, rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Ensure layout doesn't overlap\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
